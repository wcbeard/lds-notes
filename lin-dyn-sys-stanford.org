#+TITLE: Linear Dynamic Systems Notes
#+AUTHOR: Chris Beard
#+LaTeX_CLASS: myarticle
Following notes based on the [[~/Desktop/Engineering/kiet-ee-downloads/current/ee263_course_reader.pdf][EE263 Course Reader]]. Orgmode \rightarrow \LaTeXe
| [[http://www.stanford.edu/~boyd/ee263/matlab/][Matlab files]] | [[~/Dropbox/AK-MBP/edu/systems/linear-algebra-notes.pdf][Linear Algebra]] |
| [[~/Dropbox/AK-MBP/edu/systems/IntroToLinearDynamicalSys/index.html][Index]]        |                |
* Lecture 2
[[/Users/FingerMan/Dropbox/AK-MBP/edu/systems/IntroToLinearDynamicalSys/materials/lsoeldsee263/02-lin-fcts.pdf][Official Lecture Notes]]
** Interpretation for $y=Ax$
- $A$ a transformation matrix
- $y$ an observed measurement, $x$ an unknown
- $x$ is input, $y$ is output; for rows $i$ and columns $j$ in $A$, ${\bf i \rightarrow y}$, ${\bf j \rightarrow x}$
  - indexed as \bf output, input
- Lower diagonal Matrix should make you expect or have a vague thought about causality.
  - $a_{ij}=0$ for $i<j \Rightarrow y_i$ only depends on $x_1,...,x_i$ 
  - $A$ is diagonal; output only depends on input
- Sparcity pattern (block of zeroes) in a matrix should have you wonder why...usually not coincidental
- $A_{35}$ positive $\Rightarrow x_5$ increased corresponds to an increase in 3rd output, $y_3$
** Example in notes
*** Linear elastic structure
- $a_{11}$ probably positive
  - $x_1$ input gives positive push to $y_1$ output
*** Force/Torque on rigid body
- Net torque/force on body is linearly related to force inputs
*** Thermal System
- despite normally needing a Poisson equation, the steady state heat of the different locations can be represented as a linear system $Ax=y$

* Lecture 3
** Review of Linearization (affine approximation)
#+BEGIN_LATEX
  \begin{enumerate}
  \item  If $f: {\bf R}^{n} \rightarrow {\bf R}^{m} $ is differentiable at $x_{0} \in {\bf R} ^{n}$, then
  $x$ near $x_{0} \Rightarrow f(x)$ very near $f(x_{0}) + D f(x_{0})(x-x_{0})$
  where
  $$
  Df(x_{0})_{ij}= \frac{\partial f_{i}}{\partial x_{j}} \bigg|_{x_{0}}
  $$
  is the derivative (Jacobian) matrix.
  
  \item with $y=f(x), y_{0}=f(x_{0})$, define `input deviation' $\delta x := x-x_{0}$, `output deviation' $\delta y:= y-y_{0}$
  \item then we have $\delta y \approx Df(x_{0})\delta x$
  \subitem i.e., we get a linear function for looking at how the output changes with small changes in the input
  \item When deviations are small, they are approximately related by a linear function
  
  \end{enumerate}
#+END_LATEX
*** Good intuition range-finding problem
** Intepretations of $Ax=y$
- Scaled combination of columns in $A$

#+BEGIN_LATEX
  $A=[ a_1 a_{2} ... a_{n}] \rightarrow y=x_{1}a_{1}+x_{2}a_{2}+...+x_{n}a_{n}$
#+END_LATEX
- Looking at the rows: the multiplication is the inner product of rows and vector $x$
- I/O ordering is backwards in control; in $A_{ij}$, $j$ refers to input and $i$ refers to output
  - indexing is likewise backwards in block diagram indexing
#+BEGIN_LATEX
  $AB=I$; $\tilde a_{i}^{T} \cdot b_{j}=0$ if $i\ne j$, where $\tilde a_{i}$ is the $ith$ row in $A$, and $b_{j}$ is $jth$ column in $B$ 
#+END_LATEX

*** Computing $C=AB$
How to compute this faster than using formula $c_{ij}=\sum_{k=1} A_{ik} B_{kj}$: have computer break it up into submatrices and do the multiplication on them

*** Zero nullspace (in notes)
- if 0 is only element in $\mathcal{N}(A)$ 
  - $A$ is one-to-one
    - linear transformation doesn't lose information
  - columns of $A$ are independent, and basis for their span
  - $A$ has a left inverse $\Rightarrow$ you can use this to undo the transformation and find the input $x$, given the output $y$
  - $|A^T A|\ne0$ 
*** Interpretations of nullspace
supposing $z \in \mathcal{N}(A)$ 
- Measurements
  - $z$ is undetectable from sensors
  - $x$ and $x+z$ are indistinguishable from sensors: $Ax = A(x+z)$
  - the nullspace characterizes ambiguity in $x$ from measurement $y=Ax$
    - large nullspace is bad
- $y=Ax$ is output from input $x$
  - $z$ is input with no result
  - $x$ and $x+z$ have same result
  - the nullspace characterizes freedom of input choice for given result
    - large nullspace is good: more room for optimization because of more input possibilities
*** Range
Range of $A \in \mathbb{R}^{m\times n}$ defined as $\mathcal{R}(A)=\{Ax | x \in \mathbb{R}^n\} \subseteq \mathbb{R}^m$ 
- The set of vectors that can be `hit' by linear mapping $y=Ax$
- span of columns of $A$
- set of vectors $y$ for which $Ax=y$ has a solution
- possible sensor measurement
  - in design, you'll want to throw an exception if a measurement is outside the range; the sensor is bad
  - test for a bad 13th sensor: remove 13th row in $A$; if the reduced $y$ is in the range of the reduced matrix, the 13th sensor might not have been bad
**** Onto matrices
$A$ is `onto' if $\mathcal{R}(A)=\mathbb{R}^m$
- you can solve $Ax=y$ for any $y$
- columns of $A$ span $\mathbb{R}^m$
- $A$ has a right inverse $B$ s.t. $AB=I$
  - can do $ABy=A(By)=y$: you want an $x$ that gives you $y$? Here it is.
  - Design procedure
- rows of $A$ are independent
  - a.k.a., $\mathcal{N}(A^T)=\{0\}$
- $|AA^T|\ne 0$ 
**** Interpretations of range
- supposing $v \in \mathcal{R}(A)$
  - $v$ reachable
  - else, not reachable

**** Inverse
Note: square matrices are impractical for engineering. They don't let you take advantoge of redundant sensors/controllers, or let you build a robust system to take care of broken sensors
- $A \in \mathbb{R}^{n \times n}$ is invertible or nonsingular if det $A \ne 0$
  - columns of $A$ are basis for $\mathbb{R}^n$ 
  - rows of $A$ are basis for $\mathbb{R}^n$
  - $y=Ax$ has a unique solution $x$ for every $y \in \mathbb{R}^n$
  - $A$ has left and right inverse $A^{-1} \in \mathbb{R}^{n\times n}$, s.t. $AA^{-1}=A^{-1}A=I$
  - $\mathcal{N}(A)= \{0\}$
  - $\mathcal{R}(A)=\mathbb{R}^n$
  - det $A^T A= |AA^T| \ne 0$
***** Dual basis intepretation of inverse
$a_i$ are columns of $A$, and $\tilde b_i^T$ are rows of $B=A^{-1}$
- $y=Ax$, column by column, looks like $y=x_1 a_1 + ... + x_n a_n$
  - multiply both sides of $y=Ax$ by $A^{-1}=B$ gives $x=By$
  - so $x_i=\tilde b_i^T y$

\[
\begin{bmatrix}
  \vdots & \vdots & \vdots & \vdots \\
  a_1    & a_2    & ...    & a_n    \\
  \vdots & \vdots & \vdots & \vdots \\
\end{bmatrix}
\begin{bmatrix}
  x_1      \\
  x_2      \\
  \vdots   \\
  x_n      \\
\end{bmatrix}
=
\begin{bmatrix}
  y_1      \\
  y_2      \\
  \vdots   \\
  y_n      \\
\end{bmatrix}
\]

\[
x=A^{-1} y
\]

\[
\begin{bmatrix}
  x_1      \\
  x_2      \\
  \vdots   \\
  x_n      \\
\end{bmatrix}
=
\begin{bmatrix}
  \cdots & \tilde b^T_1 & \cdots \\
  \cdots & \tilde b^T_2 & \cdots \\
  \cdots & \vdots       & \cdots \\
  \cdots & \tilde b^T_n & \cdots \\
\end{bmatrix}
\begin{bmatrix}
  y_1      \\
  y_2      \\
  \vdots   \\
  y_n      \\
\end{bmatrix}
\]

\[
\begin{bmatrix}
  y_1      \\
  y_2      \\
  \vdots   \\
  y_n      \\
\end{bmatrix}
=
\begin{bmatrix}
x_1 a_1 + ... + x_n a_n
\end{bmatrix}
=
\begin{bmatrix}
(\tilde b^T_1 y) a_1 + ... + (\tilde b^T_n y) a_n
\end{bmatrix}
\]

Beautiful thing:
\[
y=\sum_{i=1}^n (\tilde b_i ^T y) a_i
\]

*** Rank of matrix
Rank of $A \in \mathbb{R}^{m \times n}$ as ${\bf rank}(A)= {\bf dim} \mathcal{R}(A)$
- ${\bf rank} (A)= {\bf rank} (A^T)$
- ${\bf rank} (A)$ is maximum number of independent columns or rows of $A$: ${\bf rank} (A) \le {\bf min} (m,n)$
- ${\bf rank} (A)+ {\bf dim} \mathcal{N}(A)=n$

**** Conservation of degrees of freedom (dimension)
- ${\bf rank} (A)$ is dimension of set `hit' by mapping $y=Ax$
- ${\bf dim} \mathcal{N}(A)$ is dimension of set of $x$ `crushed' to zero by $y=Ax$
***** Example
- $A \in \mathbb{R}^{20 \times 10} {\bf rank} (A)=8$
  - you can do 8 dimensions worth of stuff
  - 10 knobs, 2 redundant knobs, which is ${\bf dim} \mathcal{N}(A)=2$
**** Coding interpretation of rank
- rank of product: ${\bf rank} (BC) \le {\bf min} \{ {\bf rank} (B), {\bf rank} (C)\}$
- supposedly really cool stuff based on this
- low rank matrices let you do fast computations
*** Various wrap-up items
**** RMS
\[
{\bf rms} (x) = \left( \frac{1}{n} \sum^n _{i=1} \right) ^{1/2} = \frac{\| x \|}{\sqrt{n}} 
\]
**** Inner product
$\langle x,y \rangle := x_1 y_1 + x_2 y_2 + \cdots + x_n y_n = x ^{T} y$ 
- intepretation of inner product signs:
- $x ^{T} y > 0$: acute; roughly point in same direction
- $x ^{T} y > 0$: obtuse; roughly point in opposite direction
**** Orthonormal set of vectors
- set of $k$ vectors $u_1, u_2, ..., u_k \in \mathbb{R}^{n}$ orthonormal; $U= [u_1 \cdots u_k]$
- $U^T U= I_k \leftrightarrow$ set of column vectors of $U$ are orthonormal
- ${\bf warning}$: $U U ^{T} \ne I_k$ if $k<n$ 
  - say $U$ is $10\times 3$, $U^T$ is $3 \times 10$, rank of $U$ is 3 $\Rightarrow$ rank of $UU^T$ is at most 3
  - but $UU^T$ will be a $10\times 10$ matrix, so it can't be the identity matrix

* Lecture 5
A good source for more on orthogonality at [[http://www.math.umn.edu/~olver/aims_/qr.pdf][University of Minnesota]]

** Geometric properties of orthonormal vectors
- columns of $U$ are ON $\Rightarrow$ mapping under $U$ preserves distances
  - $w=Uz \Rightarrow \|w \| = \| z \|$
- Also preserves inner product
- Also preserves angles
- Something like a rigid transformation
** Orthonormal basis for $\mathbb{R}^{n}$
- if there are $n$ orthonormal vectors (remember, with dimension $n$), it forms an orthonormal basis for $\mathbb{R}^{n}$
- $U^{-1}=U^T$
  - \fbox{$U^T U=I \Leftrightarrow U$'s column vectors form an orthonormal basis for $\mathbb{R}^{n}$}
  - $\displaystyle \sum _{i=1} ^{n} u_i u_i^T = I \in \mathbb{R}^{n \times n}$ (known as a dyad, or outer product; inner products reverses the two and gives a scalar, outer gives a matrix)
  - outer products take 2 vectors, possibly of different sizes, and multiplies every combination of elements one with another
** Expansion in orthonormal basis
- $U$ orthogonal $\Rightarrow x=UU^T$
- $\displaystyle x= \sum ^{n} _{i=1} \left( u ^{T} _{i} x\right) u _{i}$
  - because $U^TU=I$, the thing in sum is really $u_i u_i^T x$
  - $u_i^T x$ is really a scalar, so this can be moved to the front of $u_i$, giving our result
  - This says $x$ is a linear combination of $u_i$'s
** Gram-Schmidt procedure
- $a_1, ..., a_k \in \mathbb{R}^{n}$ are LI; G-S finds ON vectors $q_1,..., q_k$ s.t. $$ {\bf span} (a_1,...,a_r)= {\bf span} (q_1,...,q_r)$$ for $r \le k$
- so $q_1, ..., q_r$ is an ON basis for span($a_1, ...,a_r$)
- Basic method: orthogonalize each vector wrt the previous ones, then normalize result
  1. $\tilde q_1 = a_1$
  2. normalize: $q_1 = \tilde q_1/ \|\tilde q_1 \|$
  3. remove $q_1$ component from $a_2$: $\tilde q_2 = a_2 - (q_1^T a_2) q_1$
  4. normalize $q_2$
  5. remove $q_1, q_2$ components: $\tilde q_3= a_3 - (q_1^T a_3) q_1 - (q_2^T a_3)q_2$ 
  6. normalize $q_3$ 
- $a_i= (q_1^T a_i) q_1 + (q_2^T a_i)q_2 + \cdots + (q_{i-1}^T a_i)q_{i-1} + \| \tilde q_i \| q_i$
  - $= r_{1i} q_1 + r_{2i} q_2 + \cdots + r_{ii} q_i$ ($r_{ii} \ge 0$ is the length of $\tilde q_i$)
** $QR$ decomposition
This can be written as $A=QR$, where $A \in \mathbb{R}^{n \times k}, Q \in \mathbb{R}^{n \times k} , R \in \mathbb{R}^{k\times k}$

\[
\begin{bmatrix}
  a_1 & a_2 & \cdots & a_k \\
\end{bmatrix}
=
\begin{bmatrix}
  q_1 & q_2 & \cdots & q_k \\
\end{bmatrix}
\begin{bmatrix}
  r_{11} & r_{12} & \cdots & r_{1k} \\
         0 & r_{22} & \cdots & r_{2k} \\
    \vdots & \vdots   & \ddots & \vdots   \\
         0 & 0        & \cdots & r_{kk} \\
\end{bmatrix}
\]
- $R$ triangular because computation of $a_i$ only involves up to $q_i$
  - a sort of causality, since you can calculate $q_7$ without seeing $q_8$
- Columns of $Q$ are ON basis for $\mathcal{R}(A)$
** General Gram Schmidt procedure (`rank revealing QR algorithm')
- Basically the same, but if one of the $\tilde q_i$'s is zero (meaning $a_i$ is dependent on previous $a$ vectors), then just go to the next column
- referring to notes, upper staircase notation shows which vectors are dependent on previous ones (columns without the x's)
  - entries with x are `corner' entries
** Applications
- check if $b \in {\bf span} (a_1, a2, ..., a_k)$
- Factorize matrix $A$
** Least Squares Approximation
- Overdetermined linear equation (tall, skinny, more equations than unknowns, dimensionally redundant system of equations)

* Lecture 6
On skinny, full rank matrices
** Overdetermined equations
- Skinny, more equations than unknowns
- Given $y=Ax, A\in \mathbb{R}^{m\times n}$, a randomly-chosen $y$ in $\mathbb{R}^{m}$ has 0 probability of being in the range of $A$
- To $approximately$ solve for $y$, minimize norm of error (residual) $r=Ax-y$
- find $x=x _{ls}$ (least squares approx.) that minimizes $\|r\|$
** Least Squares `Solution'
- square $\|r\|$, get expansion, set gradient wrt $x$ equal to zero
- \fbox{$x _{ls} = (A ^{T} A) ^{-1} A ^{T} y$ } $=B_{ls} y$ (linear operation)
- $A ^{T} A$ should be invertible, square, full rank
- $(A ^{T} A)^{-1} A ^{T}$ is a generalized inverse (is only inverse for square matrices, though)
  - Also known as the $A^\dagger$, `pseudo-inverse'
  - Which is a left inverse of $A$
** Projection on $\mathcal{R}(A)$
$Ax _{ls}$ is the point closest to $y$ (i.e., projection of $y$ onto $\mathcal{R}(A)$)
- $A x _{ls} = {\bf proj} _{ \mathcal{R}(A)} (y)= \left(A(A ^{T} A) ^{-1} A ^{T} \right)y$ 
** Orthogonality principle
The optimal residual is orthogonal to $C(A)$
- $r = A x _{ls} -y = (A(A ^{T} A) ^{-1} A ^{T} -I)y$ orthogonal to $C(A)$
- $\langle r, Az \rangle = y ^{T} (A(A ^{T} A) ^{-1} A ^{T} -I) ^{T} Az = 0$ for all $z \in \mathbb{R}^{n}$ 
** Least-squares via $QR$ factorization
$A$ is still skinny, full rank
- Factor as $A=QR$; $Q^TQ=I_n, R \in \mathbb{R}^{n\times n}$ upper triangular, invertible
- pseudo-inverse: $(A ^{T} A) ^{-1} A ^{T} = R ^{-1} Q ^{T} \Rightarrow$ \fbox{$R ^{-1} Q ^{T} y = x _{ls}$}
- Pretty straight-forward
- Matlab for least squares approximation
    : xl = inv(A' * A)*A'y; # So common that has shorthand in MATLAB
    : xl = A\y;                 # Works for non-skinny matrices, may do unexpected things
** Full $QR$ factorization
- $A= \begin{bmatrix}Q_1 & Q_2 \end{bmatrix} \begin{bmatrix} R_1 \\ 0 \end{bmatrix}$
  - New $Q$ is square, orthogonal matrix; $R_1$ is square, upper triangular, invertible
- Remember, multiplying by orthogonal matrix doesn't changet the norm:
  - $\| A x-y \| ^{2} = \| R_1 x - Q ^{T} _{1} y \|^2 + \| Q ^{T} _{2} y \| ^{2}$
  - Find least squares approximation with $x _{ls} = R ^{-1} _{1} Q ^{T} _{1} y$ (zeroes first term)

** Applications for least squares approximations
- if there is some noise $v$ in $y = Ax+v$
  - you can't reconstruct $x$, but you can get close with the approximation
- Estimation: choose some $\hat x$ that minimizes $\| A \hat x - y\|$, which is the deviation between the think we observed, and what we would have observed in the absence of noise
** BLUE: Best linear unbiased estimator
- $A$ still full rank and skinny; have a `linear estimator' $\hat x= By$ ($B$ is fat)
  - $\hat x = B(Ax+v)$
- Called unbiased if there is no estimation error when there's no noise; the estimator works perfectly in the absence of noise
  - if $v=0$ and $BA=I$; $B$ is left inverse/perfect reconstructor
- Estimation error uf unbiased linear estimator is $x- \hat x= sBv$, so we want $B$ to be small and $BA=I$; small means error isn't sensitive to the noise
- The pseudo-inverse is the smallest left inverse of $A$:
  - $A ^{\dagger} = (A ^{T} A) ^{-1} A ^{T}$
  - $\displaystyle \sum _{i,j} B ^{2} _{ij} \ge \sum _{i,j} A _{ij} ^{\dagger 2}$
** Range-finding example
- Find ranges to 4 beacons from an unknown position $x$
- $y = - \begin{bmatrix}
           k _{1} ^{T} \\ k _{2 } ^{T} \\ k _{3} ^{T} \\ k _{4} ^{T} 
         \end{bmatrix} x + v$
- actual position $x=(5.59, 10.58)$; measurement $y=(-11.95, -2.84, -9.81, 2.81)$
  - these numbers aren't consistent in $Ax=y$, since there's also the error; there is no such $x$ value that can give this $y$ value
- There are 2 redundant sensors (2 more $y$ values than $x$ values); one method for estimating $\hat x$ is `just enough' method: you only need 2 $y$ values; take inverse of top half of $A$ and pad the rest of the matrix with 0's
- use $\hat x = B _{just enough} y = \begin{bmatrix}\begin{bmatrix} k_1 ^{T} \\ k_2 ^{T} \end{bmatrix} ^{-1} \begin{matrix} 0 & 0 \\ 0 & 0 \end{matrix}\end{bmatrix} = \begin{bmatrix}
                                           0 & -1.0 & 0 & 0 \\ -1.12 &   .5 & 0 & 0 \\   
                                     \end{bmatrix} y = \begin{bmatrix} 2.84 \\ 11.9 \end{bmatrix}$
- Least Squares method: $\hat x A ^{\dagger} y =$ this has a much smaller norm of error
- Just enough estimator doesn't seem to have good performance...unless last two measurements were really off, since JEM only takes 2 measurements into account

** Quantizer example
Super-impressive least squares estimate; more precise than A-D converter
** Least Squares data-fitting
- use functions $f_1, f_2, ..., f_n:S \rightarrow \mathbb{R}$ are called regressors or basis functions
- applications
  - interpolation- , extrapolation, smoothing of data
| Applications                   |                                                                                  |
|--------------------------------+----------------------------------------------------------------------------------|
| interpolation                  | don't have sensors in specific location, but want the temperature                |
| extrapolation                  | get good basis functions for better interpolation                                |
| data smoothing                 | de-noise measurements                                                            |
| simple, approximate data model | Get a million samples, use the data-fitting to get a simple approximate function |

** Least-squares polynomial fitting
- Vandermonde matrix?
* Lecture 7
** Least-squares polynomial fitting, cont'd
- have data samples $(t_i, y_i), i=1,...,m$
- fit coefficients $a_i$ of polynomial $p(t)= a_0 + a_1 t + \cdots + a _{n-1} t ^{t-1}$ so that when evaluated at $t_i$ it will give you the associated $y$ value
- basis functions are $f_j(t)= t ^{j-1}, j=1,...,n$
- use Vandermonde matrix $A$ (`polynomial evaluator matrix'):
\[
A=
\begin{bmatrix}
  1 & t_1    & t_1^2 & ... & t_1^{n-1} \\
  1 & t_2    & t_2^2 & ... & t_2^{n-1} \\
    & \vdots &       &     & \vdots    \\
  1 & t_m    & t_m^2 & ... & t_m^{n-1} \\
\end{bmatrix}
\]
- side note: use this when you want to fit throughout an interval, use a Taylor series fit if you want it close to a point
** Growing sets of regressors
- Given ordered set of vectors; find best fit with first vector, then best fit with first and second, then best fit with first three...
- These vectors called /regressors/, or columns
- Say you have some /master list/ $A$ with $n$ columns, and $A ^{(p)}$ will be the matrix with the first $p$ columns of it
  - we want to minimize different sets of $\| A ^{(p)} x-y \|$
  - i.e., project $y$ onto a growing span $\{a_1, a_2, ..., a_p\}$
- Solution for each $p \le n$ given by $x _{ls} ^{(p)} = (A ^{T} _{p} A _{p} )^{-1} A _{p} ^{T} y = R ^{-1} _{p} Q ^{T} _{p} y$
  - In MATLAB, =A(:,1:p)\y=, though technically it's faster to do a sort of =for= loop
- Residual, $\| \sum ^{p} _{i=1} x_i a_i -y \|$ reduces as $p$ (number of columns) increases
  - though it may be same as residual with previous value of $p$ if the optimal $x_1=0$, when $y \perp a_1$
  - if the residual drops 15% from that of previous value of $p$, you say that $a_1$ explains 15% of $y$
** Least-squares system identification (important topic)
- measure input, output $u(t), y(t)$ for $t=0,...,N$ of unknown system, and try to get a model of system
- example: moving average (MA) model with $n$ delays (try to approximate what are the weights $h_i$ for each delay)
  - see equation/matrix in notes, though there are different ways to write it
  - get best answer with LSA
** Model order selection
- how large should $n$ be?
- the larger, the smaller prediction error on /data used to form model/
- but at a certain point, predictive ability of model on other I/O data from same system worsens
- probably best to choose the `knee' on the graph on notes slide for prediction of new data
*** Cross-validation
- check with new data, only if you're getting small residuals on data you've already seen
- when $n$ gets too large (greater than $n=10$ on graph), the error with `validation data' actually gets larger
- this example is ideal, since $n=10$ is the obvious order for the model
- *Application note*: in medical, many industries, there's a firm wall between validation data and model-developing data, so someone /else/ tests your model
- in this example, it is known as /overfit/ when the validation data error gets larger for $n$ too large
** Growing sets of measurements
- similar to GSo Regressors, except you add new rows, not columns
- this would happen if we're estimating a parameter $x$ (which is constant)
- Solution: $\displaystyle x_{ls} = \left( \sum ^{m} _{i=1} a_i a_i ^{T} \right) ^{-1} \sum ^{m} _{i=1} y_i a_i$
- new way to think of least squares equation
** Recursive ways to do least squares
- don't have to re-add for each new measurement
  - i.e., memory is bounded
  - use equation from notes; solution is $x _{ls} (m) = P(m) ^{-1} q(m)$
** Fast update algorithm for recursive LS
- Was a big deal back in the day; somewhat still
** Multi-objective least squares
- Sometimes you have 2+ objectives to minimize
  - say $J_1 = \| Ax-y\| ^{2}$ (what we've done so far)
  - and $J_2 = \| Fx-g\| ^{2}$
  - these are usually competing (minimize one at cost of other)
- Variable in question is $x \in \mathbb{R}^{n}$
- Plot in notes shows plot of $(J_1(x_i), J_2(x_i))$
- Some points are unambiguously worse than others, but there is some ambiguity when $J_1(x_1) < J_1(x_2)$, while $J_2(x_1) > J_2(x_2)$
- Fix this ambiguity with `weighted-sum objective'
- $J_1 + \mu J_2 = \| Ax-y\| ^{2} + \mu \| Fx-g\| ^{2}$
  - Say, there's a trade-off between smoothness (no noise) and better fit; $\mu$ can have different dimensions if $J_2$ does
- Use slope of $\mu$ in graph (`indifference curve', in economics) [slide 7-6]
* Lecture 8
Multi-objective least-squares
** Plot of achievable objective pairs
- if it approximates an L shape (has a `knee'), the knee is usually the obvious optimal location, so least-squares isn't as helpful
  - optimal point isn't very sensitive to \mu
- Other extreme: trade-off curve looks linear (negative slope), where it's zero-sum
  - optimal point very sensitive to \mu
  - slope commonly called /exchange rate curve/
- In this class, they must be convex curves (cup up/outward)
- To find Pareto optimal points, minimize $J_1 + \mu J_2 = \alpha$
  - on plot, can have level curves with slope \mu
  - Find point on Pareto Optimal Curve that has slope \mu
** Minimizing weighted-sum objective
- note: norm-squared of a stacked vector is norm-square of the top+norm-square of bottom
$$J_1+\mu J_2 = \| Ax - y \| ^{2} + \mu \| Fx - g \| ^{2} = \left\|
\begin{bmatrix}
  A           \\
  \sqrt \mu F \\
\end{bmatrix}
x-
\begin{bmatrix}
  y \\
  \sqrt \mu g \\
\end{bmatrix}
\right\| ^{2} 
$$
\[
= \left\| \tilde Ax- \tilde y \right\|
\]
where
\[
\tilde A =
\begin{bmatrix}
  A           \\
  \sqrt \mu F \\
\end{bmatrix}
, \tilde y =
\begin{bmatrix}
  y \\
  \sqrt \mu g \\
\end{bmatrix}
\]
If $\tilde A$ is full rank,
\begin{eqnarray}
x &=& \left( \tilde A ^{T} \tilde A \right)^{-1} \tilde A ^{T} \tilde y \\
  &=& ( A ^{T} A + \mu F ^{T} F) ^{-1} (A ^{T} y + \mu F ^{T} g)
\end{eqnarray}
Note: to plot the tradeoff curve, calculate the minimizer $x_\mu$, and plot the resulting pairs $(J_1, J_2)$ 
In MATLAB, =[A; sqrt(mu) * F]\[y;sqrt(mu) * g]=
** Example: frictionless table
- $y$ is final position at $t=10$; $y=a ^{T} x$, $a\in \mathbb{R}^{10}$ 
- $J_1 = (y-1) ^{2}$, (final position difference from $y=1$ squared)
- $J_2 = \|x\| ^{2}$ sum of force squares
- Q: Why do we often care about sum of squares? A: *It's easy to analyze* (not necessarily because it corresponds to energy)
  - max $| x_i |$ corresponds to maximum thrust
  - $\sum |x_i|$ corresponds to fuel use
- Optimal tradeoff curve is quadratic
** Regularized least-squares
- famous example of multi-objective least squares
  - second $J$ term is simply $J_2 = \|x\|$, though first is the same: $J_1 = \|Ax-y\| ^{2}$ 
- Tychonov regularization works for /any/ $A$
  - /regularized/ least-squares solution: \fbox{$x_\mu = (A ^{T} A + \mu I) ^{-1} A ^{T} y$}, for $F=I, g=0$ 
Show $(A ^{T} A + \mu I)$ is invertible, no matter what size/values of $A$ (assuming $\mu > 0$ ): 
If this is /not/ invertible (singular), it means some nonzero vector $z$ gets mapped to zero ($z \in \mathcal{N}(A)$)
\begin{eqnarray}
(A ^{T} A + \mu I) z= 0, z \ne 0 \\
z ^{T} (A ^{T} A + \mu I) z = 0 \text{ since } z^T \vec{0} =0 \\
z ^{T} A ^{T} A z + \mu z ^{T} z = 0 \\
\| A z \| ^{2} + \mu \| z \| ^{2} = 0 \\
z = \vec{0} 
\end{eqnarray}
So, $z$ can only be zero, meaning $\mathcal{N}(A) = \{0\} \Rightarrow (A ^{T} A -\mu I)$ is invertible. This is also why \mu must be positive.
Or, you know it's invertible, since it is full rank (and skinny) when you stack $\mu I$ below it (see definition of $\tilde A$).
- Application of Regularized least-squares
  - estimation/inversion
  - $Ax-y$ is sensor residual
  - prior information that $x$ is really small
  - or, model only accurate for small $x$
  - Tychonov solution trades off sensor fit and size of $x$
- Image processing example
  - Laplacian regularization
    - image reconstruction problem
  - $x$ is vectorized version of image
  - $\|A x - y\| ^{2}$ is difference from real image
  - Want new objective to minimize roughness
    - vector $Dx$ (from new matrix $D$) which has difference between neighboring pixels as elements
      - $D_v x$ measures vertical difference 
      - $D_h x$ measures horizontal difference 
      - Nullspace is vector where there is no variation between pixels
  - minimize $\|A x-y\| ^{2} + \mu \| [D_h x \; D_v x]^{T} \| ^{2}$
    - if $\mu$ is turned way up, it'll be all smoothed out
    - if you care about total size of image, you can add another parameter \lambda: $\|A x-y\| ^{2} + \mu \| [D_h x \; D_v x]^{T} \| ^{2} + \lambda \|x\| ^{2}$
** Nonlinear least squares (NLLS) problem
- find $x\in \mathbb{R}^{n}$ that minimizes $\displaystyle \| r(x) \| ^{2} = \sum ^{m} _{i=1} r _{i} (x) ^{2}$
- $r(x)$ is vector of residuals; $r(x)= Ax-y \Rightarrow$ problem reduces to linear least squares problem
- in general, can't *really* solve a NLLS problem, but can find good heuristics to get a locally optimal solution
** Gauss-Newton method for NLLS
- Start guess for $x$
- Loop
  - linearize $r$ near current guess
  - new guess is linear LS solution, using linearized $r$
  - if convergence, stop
- Linearize?
  - Jacobian: $(Dr) _{ij} = \partial r _{i} / \partial x_j$
  - Linearization: $r (x) \approx r(x ^{(k)}) + Dr(x ^{(k)} ) (x-x ^{(k)} )$
  - Set this linearized approximation equal to $r(x) \approx A ^{(k)} x-b ^{(k)}$
    - $A ^{(k)} = Dr(x ^{(k)})$
    - $b ^{(k)} = Dr (x ^{(k)}) x ^{(k)} -r(x ^{(k)})$ 
  - See rest in notes
  - At $k$ th iteration, approximate NLLS problem by linear LS problem:
    - $\| r (x) \| ^{2} \approx \left\| A ^{(k)} x-b ^{(k)} \right\| ^{2}$
      - if you wanna make this really cool add a $\mu \|x-x ^{(k)} \| ^{2}$ term on RHS
      - called a `trust region term';
      - first (original) part says to minimize sum of squares for /model/
      - trust region term says `but don't go far from where you are now'
- Could also linearize without calculus; works really well 
  - See `particle filter'
** G-N example
- Nice graph and residual plot
- As practical matter, good to run simulation several times (with different initial guesses)
- `exuastive simulation'
** Underdetermined linear equations
- $A \in \mathbb{R}^{m \times n}, m<n$ ($A$ is fat)
- more variables than equations
- $x$ is underspecified
- For this sectian *assume $A$ is full rank*
- Set of all solutions has form $\{x | Ax=y\} = \{x_p + z | z \in \mathcal{N}(A) \}$
- solution has dim $\mathcal{N}(A)= n-m$ `degrees of freedom'
  - many DOF: good for design (flexibility), bad for estimation (stuff you don't/can't know with available measurements)
** Least norm solution
- \fbox{$x _{ls} = A ^{T} (AA ^{T}) ^{-1} y$}
  - similar to our familiar skinny $A$ version: $x _{ls} = (A^{T} A) ^{-1} A ^{T} y$
  - mnemonic: $(\cdot) ^{-1}$ thing must be square
    - if $A$ skinny, both $A A ^{T} and $ $A^TA$ could be square (syntactically)
    - semantically, you need the up and down patterns that will form the smallest square, i.e., full rank matrix
* Lecture 9 pt 2
Thank you fucking Suncheon.
** General norm minimization with equality constraints
- Problem: \fbox{minimize $\|A x-b\|$ subject to $Cx=d$, with variable $x$}
- Least squares/least norm are special cases
  - Least norm: set $A=I, b=0$, then you just have norm of $x$ subject to some linear equations
- Same as: minimize $(1/2) \|Ax-b\| ^{2}$ subject to $Cx=d$
- Lagrangian is...long ugly thing...look at notes
  - a bit easier to look at block matrix format
$$
\begin{bmatrix}
  A ^{T} A & C ^{T} \\
  C        & 0      \\
\end{bmatrix}
\begin{bmatrix}
  x       \\
  \lambda \\
\end{bmatrix}
=
\begin{bmatrix}
  A ^{T} b \\
  d        \\
\end{bmatrix}
$$
- recover least squares (maybe) by eliminating $C$ from matrix (not setting to zero, but only having 1 row/column in first matrix)

** Autonomous linear dynamical systems
``What the class is nominally about''
- In continuous time, autonomous LDS has form $\dot x = Ax$
- Solution: $x(t) = e ^{ta} x(0)$
- $x(t) \in \mathbb{R}^{n}$ is called the state
  - $n$ is state dimension
- $A$ basically maps where you are ($x$) to where you're going ($\dot x$)
  - has units of s$^{-1}$, frequency
- Example illustration: vector fields
** Block diagrams
- use integrators to express $\dot x =Ax$ instead of differentiators
  - block called `bank of integrators'
  - historically used because of analog, mechanical computers
- notches to express $n$ signals
** Linear circuit example
* Lecture 10
Examples of autonomous linear dynamical systems, $\dot x = Ax$ 
** Example: Series reaction $A \rightarrow B \rightarrow C$ 
$$
\dot x=
\begin{bmatrix}
  -k_1 & 0    & 0 \\
  k_1  & -k_2 & 0 \\
  0    & k_2  & 0 \\
\end{bmatrix}
x
$$
- For second row, first term on rhs of $\dot x_2 = k_1 x_1 - k_2 x_1$ is /buildup/
- Note: Column sums are 0 implies conservation of mass/materials;
** Discrete time Markov chain
- $x(t+1) = Ax(t)$
- $x(t) = A ^{t} x(0)$
- Given current state, the matrix of /transition probabilities/ $P$ will tell you probabilities of the next state, given the current state
** Numerical integration of continuous system
- for a small time step $h$, find about where you'll be in $h$ seconds b
- $x(t+h) \approx x(t) + h \dot x(t) = (I + hA) x(t)$
- problem: when you do it for a long time, error can build up pretty high
** Higher order linear dynamical systems ($\dot x=Ax$)
$x ^{(k)} = A _{k-1} x ^{(k-1)} + \cdots + A _{1} x ^{(1)} + A _{0} x, x(t) \in \mathbb{R}^{n}$
- define new variable
$$
z =
\begin{bmatrix}
  x          \\
  x ^{(1)}   \\
  \vdots     \\
  x ^{(k-1)} \\
\end{bmatrix}
\in \mathbb{R}^{nk},
\dot z =
\begin{bmatrix}
  x ^{(1)}   \\
  \vdots     \\
  x ^{(k)} \\
\end{bmatrix}
=
\begin{bmatrix}
       0 &   I &   0 & \cdots & 0        \\
       0 &   0 &   I & \cdots & 0        \\
  \vdots &     &     &        & \vdots   \\
       0 &   0 &   0 & \cdots & I        \\
     A_0 & A_1 & A_2 & \cdots & A _{k-1} \\
\end{bmatrix}
z
$$
- `upshift $x$, and zero-pad'
- $z$ is the state, not $x$
- in notes, black diagram with chain of integrators
** Example: Mechanical systems
- Ex: $K _{12}$ is `cross-stiffness', how much stiffness you'd feel at node 1 from node 2
** Linearization near equilibrium point
Equilibrium point corresponds to constant solution ($f(x_e)= 0, x(t)=x_e$)
- if you start at an equilibrium point, you'll stay there
- if you start /near/ equilibrium point
  - veer off (unstable)
  - go towards equilibrium (stable)
  - something in between
- but, you never stay at an unstable equilibrium position, since equation is really $\dot x = f(x) + w(t)$, where $w(t)$ is noise
- Near equilibrium point, $\dot{\delta x}(t) \approx Df(x_e) \delta x(t)$, where $D$ is the Jacobian
  - similar to euler forward equation
- Don't fully trust approximations on approximations (but hope they work)
** Example: pendulum linearization
- $ml ^{2} \ddot{\theta}=-lmg \sin \theta$
- rewrite as 1st order DE with state $x=[\theta \; \dot \theta] ^{T} = [x_1 \; x_2] ^{T}$:
$$
\dot x =
\begin{bmatrix}
  x _{2}          \\
  -(g/l) \sin x_1 \\
\end{bmatrix}
$$ 
- $\exists$ equilibrium point at $x=0$ (and \pi), so we linearize system near $x _{e} =0$, using a Jacobian matrix:
$$
\dot{\delta x}=
\begin{bmatrix}
  \frac{\partial x_2}{\partial x_1}               & \frac{\partial x_2}{\partial x_2}               \\
  \frac{\partial}{\partial x_1} \left(-(g/l) \sin x_1 \right)|_{x_1=0} & \frac{\partial}{\partial x_2} (-(g/l) \sin x_1) \\
\end{bmatrix}
\delta x  
=
\begin{bmatrix}
     0 & 1 \\
  -g/l & 0 \\
\end{bmatrix}
\delta x  
$$

* Lecture 11
Solution via Laplace transform and matrix exponential
Remember, we've already overloaded $\dot x =ax$. Now, we'll overload exponentials to apply to matrices $x(t) = e ^{ta} x(0)$.
** Laplace transform
- $z: \mathbb{R} _{+} \rightarrow \mathbb{R}^{p\times q}$ (function that maps non-negative real scalars to matrices)
- Laplace transform: $Z= \mathcal{L}(z)$, defined by $\displaystyle Z(s) = \int _{0} ^{\infty} e ^{-st} z(t) dt$
- Region of convergence of $Z$ is mostly for confusing students
- Derivative property: $\mathcal{L}(\dot z) = sZ(s)-z(0)$
So, we can use the Laplace transform to solve $\dot x=Ax$. Take Laplace: $sX(s)-x(0)=AX(s)$, rewrite as $(sI-A)X(s) = x(0)$, so $X(s) = (sI-A) ^{-1} x(0)$. Then take the inverse transform: \fbox{$x(t) = \mathcal{L} ^{-1} \left( (sI-A) ^{-1} \right) x(0)$}
- takes advantage if linearity of the Laplace transform
- $(sI-A) ^{-1}$ is called the /resolvent/ of $A$
  - but not defined for eigenvalues of $A$; $s$, ST det($sI-A$)=0
- \fbox{$\Phi = \mathcal{L} ^{-1} ((sI-A) ^{-1} )$ } is called the /state-transition matrix/, which maps the initial state to state at time $t$: \fbox{$x(t) = \Phi(t)x(0)$ }
** Example: Harmonic oscillator
$$
\dot x =
\begin{bmatrix}
   0 & 1 \\
  -1 & 0 \\
\end{bmatrix}
x
$$ 
- To solve for $s$, get the resolvent, then apply the Laplacion to it /elementwise/, getting
$$
x(t) =
\begin{bmatrix}
  \cos t  & \sin t \\
  -\sin t & \cos t \\
\end{bmatrix}
x(0)
$$
Which is a circular rotation matrix. The solutions to $\dot x = ax$ is $x(t) = e ^{ta} x(0)$
- $a$ positive: exponential growth
- $a$ negative: exponential decay
- $a=0$: constant
** Example: Double Integrator
- Note, with scalars, $x$ in $\dot x=ax$ grows exponentially in time, and cannot grow linearly, as with matrices (can have a $t$ element in matrix)
- What is first column of $\Phi(t)$ say? It tells what the state trajectory is if the initial condition was $e_1$ (second column tells what it is if $x(0)= e_2$)
- First row says the linear combination that $x_1$ is at time $t$ given $x(0)$
** Characteristic polynomial
$\mathcal X(s) = {\bf det} (sI-A)$; called a /monic/ polynomial
- roots of $\mathcal X$ are eigenvalues of $A$, and $\mathcal X$ has real coefficients, so e-values are real or occur in conjugate pairs
** Get eigenvalues of $A$ and poles of resolvent
Use Cramer's rule to get $i,j$ entry:
$$
(-1) ^{i+j} \frac{\text{det} \Delta _{ij}}{\text{det}(sI-A)},
$$
where $\Delta _{ij}$ is $sI-A$ with $j$ th row and $i$ th column deleted. Poles of entries of resolvent *must* be eigenvalues of $A$.
** Matrix exponential
How to overload exponentials for matrices; start with $(I-C) ^{-1}= I + C + C ^{2} +$ ... Series converges if |eigenvalues of $C$ |<1.
Do series expansion of resolvent, then take the Laplacian of the series, which looks like the form for the expansion of $e ^{ta}$ (though square matrices replace scalars). So we end by learning that the state transition matrix, $\Phi(t)$ is the matrix exponential $e ^{tA}$.
- Many scalar exponential properties don't extend to matrix exponential; with scalars, this is wrong: $e ^{A+B} = e ^{A} e ^{B}$ (unless $A$ and $B$ commute: $AB=BA$)
- But this is ok: $e ^{-A} = (e ^{A} ) ^{-1}$
- So, how do you find the matrix exponential:
Find $e^A$,
$$
A=
\begin{bmatrix}
  0 & 1 \\
  0 & 0 \\
\end{bmatrix}
$$ 
Found $e ^{tA} = \mathcal L ^{-1} (sI-A) ^{-1}$ in earlier example, so just plug in $t=1$.
- Matlab: =expm(A)=, not elementwise =exp(A)=
** Time transfer property
Summary: for $\dot x = Ax$, $x(t) = \Phi (t)x(0) =$ \fbox{$e ^{tA} x(0)$}. \fbox{The matrix $e ^{tA}$ propagates initial condition into state at time $t$.} Also propagates backward in time if $t<0$.

If given $x(12)$, find $x(0)$ via $e ^{-12A} x(12)$.
- Can use first order forward Euler approximate state update for small $t$
- Discretized autonomous LDS: $z(k+1) = e ^{hA} z(k)$ (not an approximation for these equations)

** Application: sampling a continuous time system
* Lecture 12
Piecewise constant system: $A$ is constant for certain intervals of time.
- Qualitative behavior of $x(t)$
  - Eigenvalues determine (possible) behavior of $x$
  - Can plot eigenvalues on complex axes; like pole plot
  - Can put $x$ in summation form with polynomial coefficient and exponential terms
** Stability
- $\dot x=Ax$ is stable if $e ^{tA} \rightarrow 0$ as $t \rightarrow \infty$
  - means that state $x(t)$ converges to 0 as $t \rightarrow \infty$, no matter $x(0)$
  - all trajectories of $\dot x = Ax$ converge to 0 as $t \rightarrow \infty$
  - $\dot x=Ax$ is stable iff all eigenvalues of $A$ have negative real part
** Eigenvectors and diagonalization
- $\lambda \in \mathbb C$ is an eigenvalue of $A \in \mathbb C ^{n\times n}$ if (characteristic polynomial)
$$
\mathcal X(\lambda) = \text{det}(\lambda I-A) = 0
$$
- i.e., $(\lambda I-A)$ is singular, not invertible, $\mathcal{N}$ not equal to the 0 set
Equivalent to:
- \exists nonzero $v \in \mathbb C ^{n}$ s.t. $(\lambda I -A) v = 0$: \fbox{$Av=\lambda v$} ($v$ is the eigenvector)
  - columns are dependent
- \exists nonzero $w \in \mathbb C ^{n}$ s.t. $w ^{T} (\lambda I -A) = 0$: \fbox{$w^T A=\lambda w ^{T}$} ($w$ is the /left eigenvector/)
  - rows are dependent
- real $A$ can still have complex e-pairs
- $A,\lambda$ real \Rightarrow \lambda is associated with a real $v$
- conjugate (negate imaginary term of complex number[s])
- hermitian conjugate (and transpose)
** Scaling intepretation
$Av$ is simply scaled version of $v$ (\lambda times); all components get magnified by the same amount
** Dynamic intepretation
For $Av=\lambda v$, if $\dot x= Ax,x(0)=v$ \Rightarrow \fbox{$x(t) = e ^{\lambda t} v$} $= e ^{tA} v$.
- $A ^{2} v = \lambda ^{2} v$
- So you just need a scalar in front of the $v$ to calculate $x(t)!$
- \fbox{An eigenvector is an initial condition $x(0)$ for which the entire trajectory is really simple.}
- solution $x(t) = e ^{\lambda t} v$ is a mode of $\dot x=Ax$ (associated with eigenvalue \lambda)
** Invariant set
a set $S \subseteq \mathbb{R}^{n}$ is /invariant/ under $\dot x = Ax$ if whenever $x(t) \in S$, then $x(\tau) \in S$ for all $\tau \ge t$ (you stay stuck within the set)
- vector field intepretation: trajectories only cut /into/ $S$
If a single point is an invariant set, it must be in the nullspace; $S=\{x_0\} \Leftrightarrow x_0 \in \mathcal{N}(A)$, so $Ax_0=0=\dot x$.
- line $\{tv | t \in \mathbb{R}\}$ is invariant for eigenvector $v$
** Complex eigenvectors
- for $a \in \mathbb C$, complex trajectory $a e ^{\lambda t} v$ satisfies $\dot x = Ax$, as well as /real/ part
$$
x(t) = \text{Re}(ae ^{\lambda t} v)
$$
$$
= e ^{\sigma t}
\begin{bmatrix}
  v _{re} & v _{im} \\
\end{bmatrix}
\begin{bmatrix}
  \cos \omega t  & \sin \omega t \\
  -\sin \omega t & \cos \omega t \\
\end{bmatrix}
\begin{bmatrix}
  \alpha \\
  -\beta \\
\end{bmatrix}
$$ 
where
$$
v= v _{re} + jv _{im} , \lambda = \sigma + j \omega, a = \alpha + j \beta
$$ 
- \sigma gives logarithmic growth/decay factor
- \omega gives angular velocity of rotation in plane
- trajectory stays in /invariant plane/ span $\{v _{re} ,v _{im}\}$
** Dynamic interpretation: left eigenvectors
** Summary:
- /right eigenvectors/ are initial conditions from which resulting motion is simple (i.e., remains on line or in plane)
- /left eigenvectors/ give linear functions of state that are simple, for any initial condition
** Example- companion matrix
- Easy to get the characteristic polynomial
- General truth: with these matrices you can't generally tell the system behavior by just looking at it
- If you push a signal through an integrator, it gets less wiggly
- By multiplying by the left eigenvector, you've filtered out the sinusoid?

* Lecture 13
** Example: Markov chain
Probability vector $p \in \mathbb{R}^{n}$ that you're in each of $n$ states: $p(t+1)=Pp(t)$. This probability evolves in time by being multiplied by state transition matrix $P$.
- $p _{i} (t)= {\bf Prob} (z(t)=i) \Rightarrow \sum ^{n} _{i=1} p _{i} (t) =1$ 
- sum of each column is 1
  - called stochastic
- i.e., $[1\;1\; \cdots 1]$ is a left eigenvector of $P$ with $\lambda = 1$
- so det($I-P$)=0, so there's also a nonzero right eigenvector s.t. $Pv=v$
  - $v$ can always be chosen to have non-negative elements, and can be normalized
- *Interpretation*: $v$ is an equilibrium distribution; you don't change your /probability/ distribution in time; always in $v$ 
  - if $v$ unique, it's called the steady-state distribution of the Markov chain
** Diagonalization
- $v_1,...,v_n$ is LI set of eigenvectors of $A \in \mathbb{R}^{n\times n}$: $Av_i=\lambda_i v_i$
- Concatenate in matrix language:
$$
A
\begin{bmatrix}
  v_1 & \cdots & v_n \\
\end{bmatrix}
=
\begin{bmatrix}
  v_1 & \cdots & v_n \\
\end{bmatrix}
\begin{bmatrix}
  \lambda_1 &           &        &           \\
            & \lambda_2 &        &           \\
            &           & \ddots &           \\
            &           &        & \lambda_n \\
\end{bmatrix}
$$
or, $AT=T\Lambda$, or $T ^{-1} AT=\Lambda$
- note, $T$ is invertible, since its columns are linearly independent
- This is why, while $Av=\lambda v$ is more commonly used for a scalar eigenvalue, \fbox{$Av=v\lambda$} is more general, as it can represent a vector of eigenvalues \lambda.
- so, $A$ is diagonalizable if
  - \exists $T$ s.t. $T ^{-1} AT=\Lambda$ is diagonal
  - $A$ has a set of linearly independent eigenvectors
    - if $A$ not diagonalizable, it is called defective
** Not all matrices diagonalizable
i.e.,
$$
A=
\begin{bmatrix}
  0 & 1 \\
  0 & 0 \\
\end{bmatrix}
$$ 
** Distinct eigenvalues
*fact*: distinct eigenvalues in $A$ \Rightarrow $A$ diagonalizable
- converse not true, i.e., $I \in \mathbb{R}^{7\times 7}$
** Diagonalization and left eigenvectors
rewrite $T ^{-1} AT = \Lambda$ as $T ^{-1} A = \Lambda T ^{-1}$:
$$ 
\begin{bmatrix}
  w ^{T} _1 \\
  \vdots    \\
  w ^{T} _n \\
\end{bmatrix}
A=\Lambda
\begin{bmatrix}
  w ^{T} _1 \\
  \vdots    \\
  w ^{T} _n \\
\end{bmatrix}
$$
- remember that \Lambda is diagonal matrix, and multiplying by a diagonal matrix on the left is equivalent to scaling rows of the matrix
  - on the right scales the columns
Remeber left/right multiplication results (whether it scales columns or rows) with $2 \times 2$ matrix multiplication:
$$
\begin{bmatrix}
    2 &   0 \\
    0 &   3 \\
\end{bmatrix}
\begin{bmatrix}
  x_1 & x_2 \\
  y_1 & y_2 \\
\end{bmatrix}
=
\begin{bmatrix}
  2x_1 & 2x_2 \\
  3y_1 & 3y_2 \\
\end{bmatrix}
$$
I.e., right multiplication of diagonal matrix scales the rows.
- Take LI set of eigenvectors as columns, invert that matrix, then the rows are *left* eigenvectors
- An eigenvector is still an eigenvector after being scaled; so any can be normalized
** Modal form
Take a LI set of eigenvectors from $A$, shove them together as columns of new matrix $T$ = ``$A$ is diagonalizable by $T$''
- can define new coordinates by $x=T \tilde x$:
- $\tilde x$ is coordinates of $x$ in the $T$ expansion; modal (or eigenvector) expansion
  - $\tilde x$ is $x$ in terms of the eigenvectors
$$
T \dot{\tilde x}=AT \tilde x \Leftrightarrow \dot{ \tilde x}= T ^{-1} AT \tilde x \Leftrightarrow \dot{ \tilde x} = \Lambda \tilde x
$$
- in new coordinate system, system is diagonal (decoupled)
- normally, with $\dot x=Ax$, there's a ton of cross-gains from input $x_i$ to output $y_j$, where all the outputs depend on all the inputs (assuming $A$ has only non-zero entries)
  - \fbox{diagonalized system decouples it}; trajectory consists of $n$ independent modes:
$$
\tilde x_i (t) = e ^{\lambda_i t} \tilde x _{i} (0)
$$
** Real modal form
when eigenvalues (\Rightarrow $T$) are complex
- notes show block diagram of complex mode (note if real parts \sigma are removed, you get harmonic oscillator)
** Diagonalization simplification
Simplifies calculation of:
- resolvent
- powers ($A^k$)
- exponential ($e ^{A}=T {\bf diag} (e ^{\lambda_1},\dots, e ^{\lambda_n}) T ^{-1}$)
- So, diagonalization is largely a conceptual tool, and sometimes gives great computational advantage
** Simplify for analytical functions of a matrix
** Solution via diagonalization
$\dot x=Ax$ solution is $x(t)=e ^{tA} x(0)$
- with diagonalization, solution given as
$$
x(t) = \sum ^{n} _{i=1} e ^{\lambda_i t} (w_i ^{T} x(0))v_i
$$ 
** Interpretation
- (left eigenvectors) decompose initial state $x(0)$ into modal components $w ^{T} _{i} x(0)$
- $e ^{\lambda_i t}$ term propagates $i$ th mode forward $t$ seconds
- reconstruct state as linear combination of (right eigenvectors)
** Application
Finding $x(0)$ that gives stable solution.
** Stability of discrete-time systems
- powers of complex numbers $s^k$ go to zero if $|s|<1$
  - imaginary part tells how much of a rotation at each step you get
- \fbox{$x(t+1) = Ax(t)$ is stable iff all eigenvalues of $A$ have magnitude less than one}
- spectral radius of $A:\rho (A)= {\bf max} |\lambda _i |$
  - so it is a stable system iff $\rho(A)<1$
  - \rho gives rough growth or decay

** Jordan Canonical form
- /Any/ matrix $A \in \mathbb{R}^{n\times n}$ can be expressed in Jordan-canonical form (via `similarity transformation,' for some invertible matrix $T ^{-1}$)
$$
T ^{-1} AT=J=
\begin{bmatrix}
  J_1 &        &     \\
      & \ddots &     \\
      &        & J_q \\
\end{bmatrix}
$$ 
where
$$
J_i =
\begin{bmatrix}
  \lambda_i &         1 &        &           \\
            & \lambda_i & \ddots &           \\
            &           & \ddots &         1 \\
            &           &        & \lambda_i \\
\end{bmatrix}
\in \mathbb C ^{n_i \times n_i}
$$ 
- $J$ is `upper bidiagonal'
- Jordan form is unique (up to permutations of blocks- blocks might be in different places in the diagonal)
- /Almost/ strictly a conceptual tool; almost never used for numerical computations
- Jordan forms are inutil if the matrix is already diagonalizable
- When you get into Jordan form, you can use a chain of integrators to represent it in block diagram form
- Jordan blocks refer to dynamics blocks that cannot be decoupled
- \fbox{Jordan blocks yield:}
  - repeated poles in resolvent
  - terms of form $t ^{p} e ^{t\lambda}$ in $e ^{tA}$: \fbox{solution to $\dot x=Ax$ is in form of polynomials, with $t^n e^{\lambda t}$ terms}
** Jordan block example
$$
\dot x=
\begin{bmatrix}
  0 & 1 &   &   &   \\
    & 0 & 1 &   &   \\
    &   & 0 & 1 &   \\
    &   &   & 0 & 1 \\
\end{bmatrix}
x
$$ 
- Eigenvalues are all zero
- in the solutions you should expect $t ^{p} e^0$ ('constant') terms: solution of form $t + t ^{2} + t ^{3}$
* Lecture 14
- Jordan canonical form: `closest you can get to diagonalizable form, if matrix isn't diagonalizable'
- via inverse Laplace transform:
$$
e ^{tJ_\lambda} = e ^{t \lambda} (I+t F_1 + \cdots + (t ^{k-1} /(k-1)!) F _{k-1})
$$

$$
= e ^{t \lambda}
\begin{bmatrix}
  1 & t & \cdots & t ^{k-1}/(k-1)! \\
    & 1 & \cdots & t ^{k-2}/(k-2)! \\
    &   & \ddots & \vdots          \\
    &   &        & 1               \\
\end{bmatrix}
$$

Jordan-blocks should be associated in your mind with polynomials times $e ^{\lambda t}$ 
- All FIR filters have Jordan blocks
** J-C application: Caley-Hamilton theorem
- if proving things about matrices, first show it for diagonalizable matrices, then with Jordan-Canonical form
- for any $n\times n$ matrix $A$, the powers of $A: I,A ^{2}, A ^{3},...,A ^{n}$ span $\mathbb{R}^{n\times n}$
  - $\mathcal X (A)=0$ (characteristic polynomial; by the time you get to $n$, you're getting dependent matrices)
- wrong proof for evaluating characteristic polynomial $\mathcal X(s)= {\bf det} (sI-A)$ at $A$:
  - $\mathcal X(A)= {\bf det} (AI-A)= {\bf det} (0)=0$ 
  - wrong, since definition of $\mathcal X$ involves $sI$ term, which is diagonal matrix (so $A$ can't be plugged into it)
- correct example for
$$
A= \begin{bmatrix}
  1 & 2 \\
  3 & 4 \\
\end{bmatrix}
$$ 
\begin{eqnarray}
  \mathcal X &=& A ^{2} -5A-2I \\
&=&
\begin{bmatrix}
   7 & 10 \\
  15 & 22 \\
\end{bmatrix}
-5
\begin{bmatrix}
  1 & 2 \\
  3 & 4 \\
\end{bmatrix}
-2I \\
&=& 0
\end{eqnarray}
- \fbox{Caley-Hamilton theorem: the square of a (square) matrix is a linear combination of the Identity matrix and itself}
  - the inverse is also a linear combination of $I$ and powers of $A$:
  - rewrite C-H theorem: $\mathcal X(A)= A ^{n} + a _{n-1} A ^{n-1} + \cdots + a _{0} I=0$ as
    - $I=A\left( -(a_1 /a_0)I-(a_2 /a_0)A - \cdots -(1 /a_0) A ^{n-1} \right)$
$$
A ^{-1} =  -(a_1 /a_0)I-(a_2 /a_0)A - \cdots -(1 /a_0) A ^{n-1} 
$$ 
- Corollary: for every $p \in \mathbb Z_+$,
$$A ^{p} \in {\bf span} \{I,A,A ^{2} ...,A ^{n-1} \}$$
** Solving huge matrix equations
- usually have a method for fast matrix vector multiplication: $Az$
- if you can do a quick $Az$ calculation, you can also do a quick $A ^{2} z,...A ^{n-1} z$ calculation
  - using C-H thm, you can put scalars in front of them to get $A^{-1}$ 
** LTI system inputs & outputs
LTI systems with input and outputs have the form $\dot x=Ax+Bu, y=Cx+Du$; $Ax$ is the /drift/ term, $Bu$ is the /input/ term. For 
** Interpretations
- state derivative $\dot x$ is the sum of the autonomous term $Ax$ and one term per input, $b_i u_i$
  - if columns of $B$ are independent, each input $u_i$ gives another degree of freedom for $\dot x$
  - column $i$ of $B$ represents how much input $i$ affects $\dot x$
  - row $i$ of $B$ represents how much input $i$ affects the $i$ th component of $\dot x$
- Good block diagram in notes, pg. 13-5
** Transfer matrix
- Solution to $\dot x=Ax+Bu$ via Laplace transform is \fbox{$\displaystyle x(t)= e ^{tA} x(0) + \int ^{t} _{0} e ^{(t-\tau) A} Bu (\tau) d\tau$ }
  - $e ^{tA} x(0)$ is the (unforced or) autonomous response
  - $e ^{tA} B$ is the `input-to-state impulse matrix'
    - integral term is convolution
  - $(sI-A) ^{-1} B$ is input-to-state /transfer matrix/; the resolvent \times $B$
- Readout equation, $y=Cx+Du$ solution (via Laplacian)
  - \fbox{$y(t) = C e ^{tA} x(0) + \int ^{t} _{0} C e ^{(t-\tau)A} B u(\tau) d\tau + Du(t)$}
  - transfer function: $H(s)= C(sI-A) ^{-1} B+D$
  - impulse matrix/impulse response: $h(t)= C e ^{tA} B + D \delta(t)$
  - zero initial condition gives $Y(s)= H(s)U(s), y=h*u$
    - $H _{ij}$ is transfer function from input $u _{j}$ to output $y _{i}$
- Mnemonic for convolution intuition $\displaystyle y(t) = \int^{t}_{0} h(t-\tau) u(\tau) d\tau$:
  - output is `linear combination' or `mixture' of input in the past $u(\tau)$, weighted by how many seconds ago something happened
  - \tau is `seconds ago'
  - example: if $h(7)$ is really big \rightarrow output depends a /lot/ on the input 7 seconds ago
    - remember, the $h$ term has been delayed and flipped: $h(-(\tau-t))$ 
  - if $h(t)$ decays, the system has /fading memory/ 
** Impulse matrix
- $h(t) = C e ^{tA} B+D \delta(t), x(0)=0,y=h*u$
$$
y_i(t) = \sum ^{m} _{j=1} \int ^{t} _{0} h _{ij} (t-\tau) u _{j} (\tau) d\tau
$$ 
- for rain/river metaphor, where rain is input and river level is output, this matrix is summed over the input, with different regions of rainfall $u_j$ and different locations of the river where rainfall is measured, $y_i(t)$ 
** Step response
$$
s(t) = \int ^{t} _{0} h(\tau) d\tau
$$ 
** Spring dashpot example
** RC circuit example
* Lecture 15
** DC or static gain matrix
- transfer matrix at $s=0$ is $H(0)= -CA ^{-1} B + D \in \mathbb{R}^{m \times p}$
- DC transfer matrix describes system under static conditions: $x,u,y$ constant
  - $\Rightarrow y=H(0)u$
  - if system is stable, $\displaystyle H(0)= \int ^{\infty} _{0} h(t) dt= \lim_{t\rightarrow \infty} s(t)$
** Discretization with piecewise constant inputs
** Causality
** Idea of state
- $x(t)$ is state of system at time $t$:
  - future output depends only on /current state/ and /future output/
    - sufficient statistic of what happened in the past to be able to predict the future
  - bridge between past inputs and future outputs
    - `past and future are conditionally independent, given the state' (-machine learning )
** Change of coordinates
- start with LDS $\dot x=Ax+Bu,y=Cx+Du$
- change coordinates in $\mathbb{R}^{n}$ to $\tilde x$ with $x=T \tilde x$
$$ 
\dot{\tilde x} = T ^{-1} \dot x = T ^{-1} (Ax+Bu) = T ^{-1} AT \tilde x + T ^{-1} Bu
$$
- LDS can be expressed as
$$
\dot{\tilde x} = \tilde A \tilde x+\tilde B u, y= \tilde C \tilde x + \tilde D u
$$
where
$$ 
\tilde A = T ^{-1} A T, \tilde B= T ^{-1} B, \tilde C = CT, \tilde D=D
$$ 
- You might want to do this to, i.e., get an $A$ with a bunch of zeros, since the system might be much easier to implement
** Standard forms for LDS
Can change coordinates to put $A$ in various forms (diagonal, real modal, Jordan ...)
** Discrete-time systems
(Another block diagram in notes)
** Z-transform
** Discrete-time transfer function
- re-derivation of stuff we know, using the frequency domain
* Last Section, Singular Value Decomposition
** Eigenvalues of symmetric matrices
Given $A \in \mathbb{R}^{n\times n}:A=A ^{T}$ 
- eigenvalues always real
- $Av=\lambda v, v\ne 0,v\in \mathbb C^n$; remember conjugate transpose, $\bar v ^{T}=v ^{H} = v ^{*T}$, or, in MATLAB,
: v'
and remember, $\bar a a= |a| ^{2}$ 
$$
\Rightarrow v ^{H} Av= v ^{H} (Av) = \lambda v ^{H} v = \lambda \sum ^{n} _{i=1} |v_1| ^{2} 
$$ 
$$ 
...= \bar \lambda \sum ^{n} _{i=1} |v_1| ^{2} 
$$ 
so $\lambda = \bar \lambda \Rightarrow \lambda \in \mathbb{R}^{n}$
** Eigenvectors of symmetric matrices
*fact:* There is a set of orthonormal eigenvectors of $A$, $q_1,...,q_n: Aq_i=\lambda_i q_i, q_i ^{T} q_j = \delta _{ij}$ 
- In matrix form, \exists orthogonal $Q: Q ^{-1} AQ = Q ^{T} AQ = \Lambda$
- or, rewrite /dyadic expansion of A/:
$$
A = Q \Lambda Q ^{T} = \sum ^{n} _{i=1} \lambda (q_i q_i ^{T})
$$
- /engineering etiquette aside/: ``The eigenvectors of a symmetric matrix are orthonormal" makes no sense; any matrix has zillions of eigenvectors
  - but rather, ``you can choose the eigenvectors of a symmetric matrix to be orthonormal"
** Interpretations
Given $A=Q \Lambda Q ^{T}$. Remember, $Q ^{-1} =Q ^{T}$. (Notes have block diagram.)
This means, to multiply a vector $x$ by $A$, first multiply it by $Q^T$, then \Lambda, then $Q$
- first operation result $Q ^{T} x=Q ^{-1} x$ `resolves $x$ in the $q_i$ coordinates'
- next: simple to multiply by diagonal matrix \Lambda (simply scaling the matrix)
- last: multiplying by $Q$ `reconstitutes the output'
- \fbox{multiplying by $Q ^{T}, \Lambda, Q$ represents a `coding,' scaling, reconstruction operation }
- Application: in JPEG, do DCT transformation, quantize in middle, then decode the image in last step
- Geometrically:
  - rotate by $Q^T$
  - dilation by $A$
  - rotate back by $Q$
Decomposition (review)
$$ 
A = \sum ^{n} _{i=1} \lambda _{i} q _{i} q _{i} ^{T} 
$$
expresses $A$ as linear combination of 1-dimensional projections.

If a matrix is real and symmetric, (and the eigenvalues of a matrix are distinct,) then any set of associated eigenvectors is orthogonal (and can be normalized).
$$ 
A v _{i} =\lambda _{i} v _{i}, \|v_i\| = 1
$$
Then,
$$ 
v _{i} ^{T} (A v _{j} ) = \lambda _{j} v _{i} ^{T} v _{j} = (A v _{i} ) ^{T} v _{j} = \lambda _{i} v _{i} ^{T} v _{j} 
$$
so $(\lambda_i - \lambda_j) v _{i} ^{T} v _{j} =0$ for $i\ne j, \lambda _{i} \ne \lambda _{j}$ \fbox{$\Rightarrow v _{i} ^{T} v _{j} =0$ }
So distinct eigenvectors of a symmetric real matrix are `orthogonal'
** Example: RC circuit
* Lecture 16
** Quadratic forms
Previously, we did mappings of form $y=Ax$, or $\dot x=Ax$. We're no longer doing problems with a linear structure.
- New form is /quadratic form/
$$
f(x) = x ^{T} Ax= \sum ^{n} _{i,j=1} A _{ij} x _{i} x _{j} 
$$
- $(x ^{T} A x) ^{T} = x ^{T} A ^{T} x$
- $x ^{T} Ax= x ^{T} ((A +A ^{T} )/2)x$ is another form of quadratic form; the middle part is the average of itself (member of $A$) and its associated trasposed self
- weird etiquette: $A$ should be symmetric if you're dealing with quadratic forms
  - often may want to symmetrize it to make it safe
  - i.e., this quadratic form
$$
x ^{T}
\begin{bmatrix}
  0 & 1 \\
  0 & 0 \\
\end{bmatrix}
x= x_1 x_2
$$

$$
x ^{T}
\begin{bmatrix}
     0 & 3/2 \\
  -1/2 & 0   \\
\end{bmatrix}
x= x_1 x_2
$$
But the canonical form would have 1/2 in the 2nd and 3rd entry (row-wise).
- If $A$ is diagonal in LDS, $y=Ax$, it means there is no cross-coupling. If $A$ is diagonal in a quadratic form, it gives you a weighted sum of squares.
- *uniqueness:* if $x ^{T} Ax=x ^{T} Bx$ for all $x \in \mathbb{R}^{n}$ and $A=A^T, B = B ^{T}$, then $A=B$
** Examples of quadratic forms
- $\| B x \| ^{2} = x ^{T} (B ^{T} B)x; B ^{T} B is already symmetric$
- $\sum ^{n-1} _{i=1} (x _{i+1} - x _{i} ) ^{2}$. This measures `wiggliness' of signal; sum of squares of difference from one to the next signal
  - to get in quadratic form, multiply out: $x ^{2} _{i+1} + x ^{2} _{i} - 2 x _{i} x _{i+1}$:
$$
\begin{bmatrix}
  x_1 & x_2 & x_3 \\
\end{bmatrix}
\begin{bmatrix}
   1 & -1 &  0 \\
  -1 &  2 & -1 \\
   0 & -1 &  1 \\
\end{bmatrix}
\begin{bmatrix}
  x_1 \\
  x_2 \\
  x_3 \\
\end{bmatrix}
$$ 
- Zeros in corners of matrix, since there is no product between $x_1,x_3$
- $\| Fx \| ^{2} - \| Gx \| ^{2}$
  - $x ^{T} (F ^{T} F-G ^{T} G) x$ 
- quadratic surface: level set: $\{ x| f(x) = a\}$
- quadratic region: $\{ x|f(x) \le a \}$
  - unit ball: $\{x | x ^{T} x \le 1\}$ 
  - unit sphere: $\{x | x ^{T} x = 1\}$ 
** Inequalities for quadratic forms
If $A = A ^{T}$, it is diagonalizable: $A = Q \Lambda Q ^{T}$ with sorted eigenvalues in \Lambda
\begin{eqnarray}
  x ^{T} Ax &=& x ^{T} Q \Lambda Q ^{T} x \\
   &=& (Q^Tx) ^{T} \Lambda(Q ^{T}x) \\
   &=& \sum ^{n} _{i=1} \lambda _{i} (q ^{T} _{i} x ) ^{2}  \\
   &\le& \lambda _{1} \sum ^{n} _{i=1} (q _{i} ^{T} x) ^{2}  \\
   &=& \lambda_1 \| x \| ^{2}\\
\end{eqnarray}
- So, how big can the quadratic form be?
  - $x ^{T} Ax \le \lambda_1 x ^{T} x$
- Another important inequality
$$
\lambda_n x ^{T} x \le x ^{T} Ax \le \lambda_1 x ^{T} x
$$
- \lambda_1 sometimes called \lambda _{max}, \lambda _{n} called \lambda _{min}
- for \fbox{$A = A ^{T}$} $\in \mathbb{R}^{n\times n}$, $A$ is /positive semidefinite/ if $x ^{T} Ax \ge 0$ for all $x$
  - notation: $A\ge0$
  - iff $\lambda _{min} (A) \ge 0$: all eigenvalues nonnegative
  - $A$ is /positive definite/ if $x ^{T} Ax >0$ for all $x\ne 0$
- $A\ge 0 \Rightarrow A _{ij} \ge 0$
- if $A$ not positive or negative [semi]definite, it is /indefinite/
- matrices *can be* /incomparable/:
  - $A \ngeq B$ *and* $B \ngeq A$, where it will depend on the $x$ (depend on the direction you're looking at it from)
- $A \le B$ statement requires that both be square matrices, and probably symmetric (depending on social norms)
  - if not symmetric, it means $\frac{A+A^T}{2} \le \frac{B+B^T}{2}$ 
** Ellipsoids
- if $A ^{T} > 0$, the set $\mathcal E = \{x | x ^{T} Ax \le 1 \}$ is an /ellipsoid/ in $\mathbb{R}^{n}$, centered at 0
- semi-axes given by $s _{i} = \lambda _{i} ^{-1/2} q _{i}$
  - eigenvectors determine directions of semiaxes
  - eigenvalues determine lengths of semiaxes
  - $\sqrt{\lambda_{max} / \lambda_{min}}$ gives maximum eccentricity
- eigenvalue intuition in $\mathbb{R}^{3}$ 
  - large pancake shape \Rightarrow one large eigenvalue, two slightly smaller eigenvalues
  - long cigar shape \Rightarrow two large eigenvalues, one small eigenvalue
** Gain of matrix in a direction
For $A \in \mathbb{R}^{m\times n}$ (not necessarily symmetric anymore), for $x \in \mathbb{R}^{n}, \|Ax\| / \|x \|$ gives /amplification factor/ or /gain/ of $A$ in direction $x$, and will vary with different $x$ directions
- Questions we will ask
  - maximum gain of $A$
    - \fbox{$\displaystyle \max _{x\ne 0} \frac{\|Ax\|}{\|x\|}$}
    - /matrix norm/ or /spectral norm/, and denoted $\|A\|$ (overloading)
    - $\displaystyle \max _{x\ne 0} \frac{\|Ax\|}{\|x\|} = \max _{x \ne 0} \frac{x^TA^TAx}{\|x\|^2} = \lambda _{\max} (A ^{T} A)$
    - so, $\|A \|= \sqrt{\lambda _{\max} (A ^{T} A)}$ 
  - minimum gain of $A$
    - $\displaystyle \min _{x\ne 0} \|Ax\| / \|x\| = \sqrt{\lambda _{\min} (A ^{T} A)}$ 
  - how does gain of $A$ vary with direction
- Will give us a /quantitative way/ to talk about nullspace
** Example
** Properties of matrix norm
- triangle inequality
- scaling
- definiteness: $\|A\|= 0 \Leftrightarrow A = 0$
- norm of a product inequality
* Lecture 17
** SVD
Singular value decomposition, for /any/ matrix $A$.
$$
A = U\Sigma V ^{T}
$$
where 
- $A \in \mathbb{R}^{m\times n}, {\bf Rank} (A) = r$
- $A \in \mathbb{R}^{m\times r}, U ^{T} U = I$
- $V \in \mathbb{R}^{n\times r}, V ^{T} V = I$
- $\Sigma = {\bf diag} (\sigma _{1} ,..., \sigma _{r} )$, where $\sigma _{1} \ge ... \ge \sigma _{r} > 0$
** Dyadic expansion: ($U$ and $V$ have orthonormal columns)
$$
A = U \Sigma V ^{T} = \sum ^{r} _{i=1} \sigma _{i} u _{i} v _{i} ^{T} 
$$
- $\sigma _{i}$ are the nonzero /singular values/ of $A$ 
- $v_i$ are the /right/ or /input singular vectors/ of $A$
- $u _{i}$ are the /left/ or /output singular vectors/ of $A$
$$
A ^{T} A = (U \Sigma V ^{T} )^{T} (U \Sigma V ^{T}) = V \Sigma ^{2} V ^{T} 
$$
- $v _{i}$ are eigenvectors of $A ^{T}A$ corresponding to nonzero eigenvalues

- $\sigma _{i} = \sqrt{\lambda _{i} (A ^{T} A) }$ (and $\lambda _{i} (A ^{T} A) = 0$ for $i>r$)
- $\|A\| = \sigma _{1}$ 
$$
AA ^{T} = (U \Sigma V ^{T} ) (U \Sigma V ^{T} )^{T} = U \Sigma ^{2} U ^{T} 
$$
- $u _{i}$ are eigenvectors of $AA ^{T}$ corresponding to nonzero eigenvalues
- $\sigma _{i} = \sqrt{\lambda _{i} (AA ^{T}) }$ (and $\lambda _{i} (AA ^{T}) = 0$ for $i>r$)

- $u _{1} ,..., u _{r}$ are orthonormal basis for range($A$)
- $v _{1} ,..., v _{r}$ are orthonormal basis for $\mathcal{N}(A) ^{\perp}$
** Interpretations
$A = U \Sigma V ^{T}$

linear mapping $y=Ax$ can be decomposed as
- compute coefficients of $x$ along input directions $v_1,...,v_r$
- scale coefficients by $\sigma _{i}$
- reconstitute along output directions $u _{1} ,..., u _{r}$
- $u _{1}$ is highest gain output direction
- $v_1$ is most sensitive (highest gain) input direction
- $A v_1 = \sigma_1 u_1$ 

SVD gives clearer picture of gain as function of input/output directions.
** General pseudo-inverse
- $A\ne 0\Rightarrow A=U\Sigma V ^{T}$
General pseudo-inverse: \fbox{$A ^{\dagger} = V \Sigma ^{-1} U ^{T}$} is the pseudo-inverse of $A$.

If $A$ is skinny and full rank,
$$
A ^{\dagger} = (A ^{T} A) ^{-1} A ^{T} 
$$
If $A$ is fat and full rank,
$$
A ^{\dagger} = A ^{T} (AA ^{T}) ^{-1} 
$$
In the general case, (if $A$ is not full-rank)
$$
X _{ls} = \{ z: \|Az-y\|= \min _{w} \|Aw-y\| \}
$$
is the set of least squares solutions.

$x _{pinv} = A ^{\dagger} y\in X _{ls}$ has minimum norm on $X _{ls}$, so this is the minimum-norm, least squares solution
** Pseudo-inverse via regularization
Minimizer of $\|Ax-y\| ^{2} + \mu \|x\| ^{2}$ is given as
$$
x _{\mu} = (A ^{T} A + \mu I) ^{-1} A ^{T} y
$$

* Lecture 18
** Sensitivity of linear equations to data error
To perturbations $\delta x,\delta y$. Looking at /relative error/ gives us
$$
\frac{\|\delta x\|}{\|x\|} \le \|A\| \|A ^{-1} \| \frac{\| \delta y \|}{\|y\|} 
$$

$$
\kappa(A) = \text{ cond}(A) = \|A \| \|A ^{-1} \| = \sigma _{\max} (A) /\sigma _{\min} (A)
$$
So, the relative error in solution $x \le$ condition number \cdot relative error in data $y$. In terms of # of bits of guaranteed accuracy, `# of bits accuracy in solution \approx # bits accuracy in data $-\log _{2} \kappa$.'
** Low rank applications
If you want to decompose $A$ into $A=BC$, you can remember $QR$ factorization (does skinny-fat factorization), but we'd also want an $A$ that is approximated by such a factorization. Suppose $A \in \mathbb{R}^{m\times n}, {\bf Rank} (A)= r$ with SVD $A=U\Sigma V ^{T} = \sum _{i=1} ^{r} \sigma _{i} u _{i} v _{i} ^{T}$, and we want $\hat A: {\bf Rank} (\hat A) \le p<r: \|A-\hat A\|$ is minimized.

The optimal rank $p$ approximator of $A$ is
$$
\hat A=\sum ^{p} _{i=1} \sigma _{i} u _{i} v _{i} ^{T} 
$$
- $\|A-\hat A\|= \| \sum ^{r} _{i=p+1} \sigma _{i} u _{i} v _{i} ^{T} \| = \sigma _{p+1}$
- SVD dyads $u _{i} v _{i} ^{T}$ are ranked in order of `importance'; take $p$ to get rank $p$ approximant
- keep in mind Frobenius norm $\|A-\hat A\| _{F} = \| A(i)- \hat A(i)\| = \sqrt{\sum _{i,j} (A _{ij} - \hat A _{ij} ) ^{2} }$
- If you have a set of 100 $\mathbb{R}^{10}$ prices that has dimension=3, it means there are 3 underlying factors; this would give you `stunning' predictive power
- You would /almost/ (i.e., practically) have a dimension 3 matrix if the SVD \Sigma had 3 singular values that were significantly larger than the rest
** State transfer
** Reachability
Consider state transfer from $x(0)= 0$ to $x(t)$ 
- $x(t)$ is reachable in $t$ seconds
- define $\mathcal R _{t} \subseteq \mathbb{R}^{n}$ as the set of points reachable in $t$ seconds for CT system $\dot x=Ax+Bu$,
$$
\mathcal R _{t} = \left\{\int ^{t} _{0} e ^{(t-\tau)A} Bu(\tau) d\tau \big| u:[0,t] \rightarrow \mathbb{R}^{m} \right\}
$$
and for DT system $x(t+1)= Ax(t) + Bu(t)$,
$$
\mathcal R _{t} = \left\{\sum ^{t-1} _{\tau=0} A ^{t-1-\tau} Bu(\tau) \big| u(t) \in \mathbb{R}^{m} \right\}
$$
If a state is reachable in 1 s, it's reachable in 2 s (just leave it at zero for the first second) if you're starting from zero initial state (in both CT and DT systems).
** Reachability for discrete-time LDS
DT system $x(t+1)= Ax(t)+Bu(t), x(t) \in \mathbb{R}^{n}$
$$
x(t)= \mathcal C_t
\begin{bmatrix}
  u(t-1) \\
  \vdots \\
  u(0)   \\
\end{bmatrix}
$$
where $\mathcal C_t = [B \; Ab \; \cdots \; A ^{t-1} B]$. So reachable set at $t$ is $\mathcal{R}_t = \text{range} (\mathcal C_t)$

\fbox{If we want to see if a state is reachable in 1 step, see if it's in the range of $B$.}

\fbox{If we want to see if it's reachable in 2 steps, see if it's in the range of $B,AB$.}
Caley-Hamilton theorem lets us express each $A ^{k}$ for $x\ge n$ as linear combination of $A ^{0} ,...,A ^{n-1}$.
* Lecture 19
But, you can't increase your reach after $t \ge n$ seconds (or epochs). I.e.,
$$
\mathcal R_t = \left\{
\begin{matrix}
  {\bf range} (\mathcal C_t) & \quad t<n    \\
  {\bf range} (\mathcal C)   & \quad t\ge n \\
\end{matrix}
\right.
$$
where $\mathcal C=\mathcal C _{n}$ is the /controllability matrix/.
** General state transfer
$$
x(t _{f}) = A ^{t_f-t_i} x(t_i) + \mathcal C_{t_f-t_i}
\begin{bmatrix}
  u(t_f-1) \\
  \vdots   \\
  u(t_i)   \\
\end{bmatrix}
$$
- First term on rhs is what would happen without any input (the drift term)
so we can transfer $x(t_i)$ to $x(t_f)=x _{des}$ iff
$$
x _{des} -A ^{t_f-t_i} x(t_i) \in \mathbb{R} _{t_f-t_i} 
$$
- we're not hoping $x_{des}$ is reachable, we want it minus the drift
  - So it is possible to be able to reach a state in 4 steps, but not in 5
- This reduces to the reachability problem
- if system is controllable, any state transfer can be achieved in \le /n/ steps
- important special case: driving state to zero
** Minimum time control problem
Given
$$
x(t+1) = Ax(t) + Bu(t), \quad x(0) = x_0
$$
how do you minimize $t$? Check if $A ^{t} x_0 \in \mathcal R ([B \; \cdots \; A ^{t-1} B])$ 
** Least-norm input for reachability
assume system is reachable, ${\bf Rank} (\mathcal C_t) = n$ to steer $x(0)=0$ to $x(t)= x _{des}$, inputs $u(0),...,u(t-1)$ to satisfy
$$
x _{des} = \mathcal C _{t}
\begin{bmatrix}
  u(t-1) \\
  \vdots \\
  u(0)   \\
\end{bmatrix}
$$
among all $u$ that steer $x(0)= 0$ to $x(t)= x _{des}$, the one that minimizes
$$
\sum ^{t-1} _{\tau=0} \| u(\tau) \| ^{2}
$$
The least-square solution for the input is given by
$$
\begin{bmatrix}
  u _{ln}(t-1) \\
  \vdots       \\
  u _{ln}(0)   \\
\end{bmatrix}
= \mathcal C ^{T} _{t} (\mathcal C _{t} \mathcal C^{T} _{t} ) ^{-1} x _{des} 
$$
** Stability
** Continuous time reachability
consider $\dot x = Ax+Bu$ with $x(t) \in \mathbb{R}^{n}$

reachable set at time $t$ is
$$
\mathcal R _{t} = \left\{ \int ^{t} _{0} e ^{(t-\tau)A} Bu(\tau) d\tau \left| u: \right. [0,t] \rightarrow \mathbb{R}^{m} \right\}
$$
for $t>0$, $\mathcal R _{t} = \mathcal R = {\bf range} (\mathcal C)$ where
$$
\mathcal C = [B \; AB \; \cdots \; A ^{n-1} B]
$$
is the controllability matrix of $(A,B)$
- According to the model, you can hit anything infinitely fast
** Impulsive inputs
** Example
** Least-norm input for reachability
Assume that $\dot x=Ax+Bu$ is reachable. We want $u$ that steers $x(0)=0$ to $x(t)=x _{des}$ and minimizes
$$
\int ^{t} _{0} \| u(\tau) \| ^{2} d\tau
$$
let's discretize system with interval $h=t/N$ (eventually let $N\rightarrow \infty$). So $u$ is piecewise constant:
$$
u(\tau) = u _{d} (k) \text{ for } xh \le \tau < (k+1)h,\quad k=0,...,N-1
$$
...Get long expression for least norm solution for $u$ 
* Lecture 20
** Observability and state estimation
Consider DT system
$$
x(t+1) = Ax(t) + Bu(t) + w(t), \quad y(t)= Cx(t) +Du(t) + v(t)
$$
- $w$ is state disturbance/noise
- $v$ is sensor noise/error
- state estimation problem: estimate $x(s)$ from
$$
u(0),...,u(t-1), \quad y(0), ..., y(t-1)
$$
- $s=0$: estimate initial state
- $s=t-1$: estimate current state
- $s=t$: estimate/predict next state
- estimate $\tilde x(s)$, called /observer/ or /state estimator/
** Noiseless case
Find $x(0)$, with no state or measurement noise
$$
x(t+1) = Ax(t) + Bu(t), \quad y(t)= Cx(t) + Du(t)
$$
then we have
$$
\begin{bmatrix}
  y(0)   \\
  \vdots \\
  y(t-1) \\
\end{bmatrix}
=\mathcal O _{t} x(0) + \mathcal T _{t}
\begin{bmatrix}
  u(0)   \\
  \vdots \\
  u(t-1) \\
\end{bmatrix}
$$


where
$$
\mathcal O_t =
\begin{bmatrix}
  C         \\
  CA        \\
  \vdots    \\
  CA ^{t-1} \\
\end{bmatrix}
\quad
\mathcal T _{t} =
\begin{bmatrix}
  D          & 0          & \cdots &        &   \\
  CB         & D          & 0      & \cdots &   \\
  \vdots     &            &        &        &   \\
  CA ^{t-2}B & CA ^{t-3}B & \cdots & CB     & D \\
\end{bmatrix}
$$
- $\mathcal O _t$ maps initial state into resulting output over $[0,t-1]$
- $\mathcal T_t$ maps input to output over $[0,t-1]$ 
** Observability matrix
** Least-squares observers
** Parting thoughts
*** Linear algebra
*** Levels of understanding
- High school: 17 variables, 17 equations \rightarrow usually has unique solution
  - 80 vars, 60 eqns \rightarrow probably 20 extra degrees of freedom
- Platonic view (`math')
  - singular, rank, range, nullspace, Jordan form, controllability
  - stuff is true or false
- Quantitative
  - based on least-squares, SVD
  - gives numerical measures for ideas like singularity, rank, etc
  - interpretation depends on (practical) context
  - very useful in practice

* Appendix
Some special things to remember.
** Inverse, transpose properties
- $(AB) ^{-1} = B ^{-1} A ^{-1}$
- $(A ^{-1}) ^{T} = (A ^{T}) ^{-1}= A ^{-T}$ 
** Invertibility implications
For an $n$-by-$n$ matrix $A$ 
| Invertible                               | mnemonic                                                                                    |
|------------------------------------------+---------------------------------------------------------------------------------------------|
| $\vert A\vert \ne 0$                     | $\vert A\vert = 0$ \Rightarrow you can't compute the inverse                                |
|                                          | - (remember base case 2 \times 2 matrix inverse involves $1/\vert A\vert$ term)             |
| non-singular                             | singular \Rightarrow the matrix sends a nontrivial subspace to the singular subspace, \{0\} |
| $A$ is full rank                         | linearly independent columns (invertibility \Rightarrow 1-to-1/injective)                   |
| $\mathcal{N}(A)=\{0\}$                   | linearly independent columns                                                                |
| $\mathcal R (A)= \mathbb{R}^{n}$         | linearly independent columns                                                                |
| $Ax=b$ has unique solution for every $b$ | - no more than one solution (can't add members of $\mathcal N (A)$ for multiple $b$)        |
|                                          | - one solution, since $\mathcal R (A)= \mathbb{R}^{n}$; everything reachable/surjective     |
|                                          | - one solution found using the unique inverse of /A/                                        |
| rref($A)=I_n$                            |                                                                                             |
| $A$ is a product of elementary matrices  |                                                                                             |
** Tips
- to sum up all the elements in matrix $A$, multiply by one vectors: $\displaystyle \sum _{i,j} (A) _{i,j} = {\bf 1} ^{T} A {\bf 1}$ 
- each row $i$ in $Z$ is a linear combination of rows $i,...,n$ in $Y: Z=UY$, where $U$ is upper triangular: $U _{ij} =0$ for $i>j$
  - more in 2.24, hw2
- $E _{i,j} = e _{i} e _{j} ^{T} \in \mathbb{R}^{n\times n}, i,j=1,...,n$ is $n\times n$ matrix with a 1 in $i,j$ th entry, and zero elsewhere
  - has $n^2$ dimensions
- When you do a polynomial of a similarity matrix, you can pull out the outer matrices: $\mathcal X(T\Lambda T ^{-1} ) = T \mathcal X(\Lambda) T ^{-1}$
- If $AB=0$, and each is full rank, (neither is zero), then $\mathcal R (B) \subseteq \mathcal N (A)$
- $A ^{k} = 0, k\ge 1\Rightarrow$ eigenvalues of $A$ are zero. If $A$ is diagonalizable, then $A ^{k}$ can be expressed as $T \Lambda ^{k} T ^{-1}$, where \Lambda is diagonal, consisting of the eigenvalues multiplied by the $k$ th power, since $T$ is non-singular.
  - ${\bf Rank}(A)=r \Rightarrow A \in \mathbb{R}^{n\times n}$ has exactly $r$ non-zero eigenvalues
- Every eigenvalue of $AB$ is an eigenvalue of $BA$, if $A,B \in \mathbb{R}^{n\times n}$: Given \lambda is eigenvalue of $AB:ABv=\lambda v, BA(Bv)=B(AB)v=\lambda Bv$. Thus $Bv$ is an eigenvector of $BA$ assocated with \lambda
- To check whether \fbox{$x\in \mathcal R(A)$}, check whether \fbox{$rank([A \;x])= rank(A)$ }
- Least squares approximation that minimizes $x$ in $Ax=y, x _{ls} = (A ^{T} A) ^{-1} A ^{T} y$ is given in MATLAB by =xls=A\y;=
  - only works if $A$ is skinny or square
  - compute using QR (economy) factorization with
    : [Q,R]=qr(A,0); % compute economy QR decomposition
    : xls=R\(Q'*y);
- Least norm solution, $x _{ln} = A ^{T} (AA ^{T} ) ^{-1} y$, can be computed by =xln=A'*inv(A*A')*y;=
  - $A$ cannot be full rank or skinny
  - via QR factorization:
	: [Q,R]=qr(A',0);
	: xln=Q*(R'\y);
    - for more, see [[~/Dropbox/AK-MBP/edu/systems/IntroToLinearDynamicalSys/materials/lsoeldsee263/Additional5-sle_matlab.pdf][here]]
** Unrelated stuff
*** Series
\begin{eqnarray}
 S &=&\sum ^{N} _{k=0} a ^{k} =a ^{0} + a ^{1} + ...+ a ^{N} \\
 aS &=&\sum ^{N} _{k=0} a ^{k} =a ^{1} + a ^{1} + ...+ a ^{N+1} \\
 aS-S &=& -1+a ^{N+1} \\
 S &=& \frac{a^{N+1}-1}{a-1}
\end{eqnarray}
*** Taylor/Mclauren
Intuition for approximating a function at a point (say at $x=0$): assuming we know $f(0),f'(0),f''(0),f'''(0)$, put into polynomials
\begin{eqnarray}
  p_0(x) &=& f(0) \\
  p_1(x) &=& f(0) + f'(0)x \\
  p_2(x) &=& f(0) + f'(0)x + \frac{1}{2} f''(0)x^2 \\
  p_3(x) &=& f(0) + f'(0)x + \frac{1}{2} f''(0)x^2 + \frac{1}{2} \frac{1}{3} f'''(0) x^3 \\
\end{eqnarray}
Our polynomial $p_n(x)$ will both have the same value at $x=0$, as well as the same /n/ th derivatives when evaluated at 0. I.e.,
\begin{eqnarray}
  p_2'(x) &=& f'(0) + f''(0) x \\
  p_2'(0) &=& f'(0) \\
\end{eqnarray}
In general, the $n$ th order McLauren approximation polynomial of $f(x)$ at point $x_1$ will be given as
$$
f(x) \approx f(x_1) + f'(x_1)x + \frac{1}{2} f''(x_1) x ^{2} + \frac{1}{2\cdot 3} f ^{(3)} x ^{3} + ... + \frac{1}{n!} f ^{(n)} x ^{n} 
$$ 
In general, the $n$ th order Taylor approximation polynomial of $f(x)$ at point $c$ will be given as
$$
f(x) \approx f(c) + f'(c)(x-c) + \frac{1}{2} f''(c) (x-c) ^{2} + ... + \frac{1}{n!} f ^{(n)} (x-c) ^{n} 
$$ 
So when you evaluate derivatives at $x=c$, the higher order terms will still drop to zero, leaving you with the exact derivative value.
**** Cosine McLauren (around 0)
$$
\cos (x) = 1- \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + ...
$$ 
**** Sine McLauren (around 0)
$$
\sin(x) = x- \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + ...
$$ 
**** e McLauren (around 0)
$$
e ^{x} = 1+x + \frac{x^2}{2!} + \frac{x^3}{3!} + ...
$$ 
* Homework assignments
| * | [[~/Desktop/Engineering/eng-control-and-cv/stanford hw/hw1sol.pdf][Homework 1]] | Lecture 4  | 2.12.4, 2.6, 2.9, 2.12, +                                               |
| * | [[~/Desktop/Engineering/eng-control-and-cv/stanford hw/hw2sol.pdf][Homework 2]] | Lecture 6  | 3.2, 3.3, 3.10, 3.11, 3.16, 3.17, +                                      |
| * | [[~/Desktop/Engineering/eng-control-and-cv/stanford hw/hw3sol.pdf][Homework 3]] | Lecture 8  | 2.17, 3.13, 4.14.3, 5.1, 6.9, +                                         |
| * | [[~/Desktop/Engineering/eng-control-and-cv/stanford hw/hw4sol.pdf][Homework 4]] | Lecture 10 | 5.2, 6.2, 6.5, 6.12, 6.14, 6.26, 7.3, 8.2                                |
| * | [[~/Desktop/Engineering/eng-control-and-cv/stanford hw/hw5sol.pdf][Homework 5]] | Lecture 13 | 10.2, 10.3, 10.4, +                                                      |
| * | [[~/Desktop/Engineering/eng-control-and-cv/stanford hw/hw6sol.pdf][Homework 6]] | Lecture 14 | 9.9, 10.5, 10.6, 10.8, 10.14, 11.3, and 11.6a                            |
| * | [[~/Desktop/Engineering/eng-control-and-cv/stanford hw/hw7sol.pdf][Homework 7]] | Lecture 16 | 10.9, 10.11, 10.19, 11.13, 12.1, 13.1, +                                 |
| * | [[~/Desktop/Engineering/eng-control-and-cv/stanford hw/hw8sol.pdf][Homework 8]] | Lecture 18 | 13.17, 14.2, 14.3, 14.4, 14.6, 14.8, 14.9, 14.11, 14.13, 14.21, 14.33, + |
|   | [[~/Desktop/Engineering/eng-control-and-cv/stanford hw/hw9sol.pdf][Homework 9]] | Lecture 20 | 14.16, 14.26, 15.2, 15.3, 15.6, 15.8, 15.10, and 15.11                   |
